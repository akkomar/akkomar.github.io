---
published: true
title: Running Spark 2.0 on Cloudera Hadoop
layout: post
tags: [spark, hadoop]
---
Sometimes we might need to run latest Spark applications on Hadoop distribution which does not support it via its package channel (e.g. Cloudera reserves few months for testing). That's the case with recently released Spark 2.0

Since we can run Spark applications on YARN, we can use any version of Spark (read: latest) for submitting applications to existing cluster.

Here's a list of steps we need to execute to manually set up Spark distribution with existing Hadoop cluster:

1. Download / build Spark

        ./dev/make-distribution.sh --name custom-spark --tgz -Phadoop-2.6 -Phive -Phive-thriftserver -Pyarn
        
1. Go to unpacked distribution directory and copy needed, current configuration files here:

        cp -R /etc/spark/conf/* conf/
        cp /etc/hive/conf/hive-site.xml conf/
        cp /etc/hive/conf/core-site.xml conf/
        cp /etc/hive/conf/hdfs-site.xml conf/
        
1. Update spark-env:

        sed -i "s#\(.*SPARK_HOME\)=.*#\1=$(pwd)#" conf/spark-env.sh

1. Copy Spark libraries to the cluster (local YARN nodes' local filesystems or HDFS, here we'll use the latter):
  
        cd  $SPARK_HOME
        hdfs dfs mkdir /some/path/spark-2.0.0-bin-hadoop-hive
        hdfs dfs -copyFromLocal jars/* spark-2.0.0-bin-hadoop-hive
  
1. Update spark-defaults.conf
  * Set spark.master to yarn:
  
          sed -i 's/spark.master=yarn-client/spark.master=yarn/' conf/spark-defaults.conf
  
  * Delete spark.yarn.jar from spark-defaults.conf:
  
          sed '-i /spark.yarn.jar/d' conf/spark-defaults.conf
  * Set spark.yarn.jars path to previously created library directory: 
  
          echo "spark.yarn.jars=hdfs:///some/path/spark-2.0.0-bin-hadoop-hive/*" >> conf/spark-defaults.conf

1. Et voil√†! We can test new Spark installation:

        cd  $SPARK_HOME
        bin/spark-shell --master yarn
        bin/run-example SparkPi 10 --master yarn
        
        




